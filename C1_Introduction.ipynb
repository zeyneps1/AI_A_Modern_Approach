{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "396e3872-7d98-4731-b5c5-3becc1bfb466",
   "metadata": {},
   "source": [
    "# Artifical Intelligence: A Modern Approach\n",
    "\n",
    "## Chapter 1: Introduction\n",
    "\n",
    "### 1.1 What is AI?\n",
    "\n",
    "Some researchers have defined intelligence in terms of fidelity to human performance, while others prefer an abstract, formal definition of intelligence called **rationality**—loosely speaking, doing the “right thing.”\n",
    "\n",
    "The subject matter itself also varies: some consider intelligence to be a property of internal thought processes and reasoning, while others focus on intelligent behavior, an external characterization. (Machine learning is a subfield of AI that studies the ability to improve performance based on experience.)\n",
    "\n",
    "From these two dimensions—**human vs. rational** (We are not suggesting that humans are “irrational”, we are merely conceding that human decisions are not always mathematically perfect.) and **thought vs. behavior**—there are four possible combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1856110-77aa-4369-8abf-7651514edac6",
   "metadata": {},
   "source": [
    "#### 1.1.1 Acting humanly: The Turing test approach\n",
    "\n",
    "The Turing test, proposed by **Alan Turing (1950)**, was designed as **a thought experiment**. A computer passes the test if a human interrogator, after posing some written questions, cannot tell whether the written responses come from a person or from a computer.\n",
    "\n",
    "- **natural language processing** to communicate successfully in a human language;\n",
    "- **knowledge representation** to store what it knows or hears;\n",
    "- **automated reasoning** to answer questions and to draw new conclusions;\n",
    "- **machine learning** to adapt to new circumstances and to detect and extrapolate patterns.\n",
    "\n",
    "Other researchers have proposed **a total Turing test**, which requires interaction with objects and people in the real world.\n",
    "\n",
    "- **computer vision and speech recognition** to perceive the world;\n",
    "- **robotics** to manipulate objects and move about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246be85-3ca7-4efb-b12e-64e7e7180932",
   "metadata": {},
   "source": [
    "#### 1.1.2 Thinking humanly: The cognitive modeling approach\n",
    "\n",
    "We can learn about human thought in three ways:\n",
    "• **introspection**—trying to catch our own thoughts as they go by;\n",
    "• **psychological experiments**—observing a person in action;\n",
    "• **brain imaging**—observing the brain in action.\n",
    "\n",
    "The interdisciplinary field of **cognitive science** brings together computer models from AI and experimental techniques from psychology to construct precise and testable theories of the human mind.\n",
    "\n",
    "#### 1.1.3 Thinking rationally: The “laws of thought” approach\n",
    "\n",
    "Aristotle's syllogism, logic, logicist, probability\n",
    "\n",
    "#### 1.1.4 Acting rationally: The rational agent approach\n",
    "\n",
    "Computer agents are expected to do: operate autonomously, perceive their environment, persist over a prolonged time period, adapt to change, and create and pursue goals.\n",
    "\n",
    "**A rational agent** is one that acts so as to achieve the best outcome or, when there is uncertainty, the best expected outcome.\n",
    "\n",
    "The rational-agent approach to AI has two advantages over the other approaches. First, it is more general than the “laws of thought” approach because correct inference is just one of several possible mechanisms for achieving rationality. Second, it is more amenable to scientific development.\n",
    "\n",
    "In a nutshell, AI has focused on the study and construction of agents that **do the right thing**. What counts as the right thing is defined by the objective that we provide to the agent. This general paradigm is so pervasive that we might call it the standard model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac93d1a6-1ab7-4f4f-9b00-869fb9d7cb8a",
   "metadata": {},
   "source": [
    "### 1.1.5 Beneficial machines\n",
    "\n",
    "The standard model is probably not the right model in the long run. The standard model assumes that we will supply a fully specified objective to the machine.\n",
    "\n",
    "The problem of achieving agreement between our true preferences and the objective we put into the machine is called the **value alignment problem**: the values or objectives put into the machine must be aligned with those of the human.\n",
    "\n",
    "We don’t want machines that are intelligent in the sense of pursuing their objectives; we want them to pursue our objectives. If we cannot transfer those objectives perfectly to the machine, then we need a new formulation—one in which the machine is pursuing our objectives, but is necessarily uncertain as to what they are. When a machine knows that it doesn’t know the complete objective, it has an incentive to act cautiously, to ask permission, to learn more about our preferences through observation, and to defer to human control. Ultimately, we want agents that are **provably beneficial** to humans.\n",
    "\n",
    "## 1.2 The Foundations of Artificial Intelligence\n",
    "\n",
    "Descartes, **dualism**. Distinction between mind and matter. There is a part of the human mind (or soul or spirit) that is outside of nature, exempt from physical laws.\n",
    "\n",
    "An alternative to dualism is **materialism**, which holds that the brain’s operation according to the laws of physics constitutes the mind. The terms **physicalism and naturalism** are also used to describe this view that stands in contrast to the supernatural.\n",
    "\n",
    "Given a physical mind that manipulates knowledge, the next problem is to establish the\n",
    "source of knowledge. The **empiricism** movement, “Nothing is in the understanding, which was not first in the senses.”\n",
    "\n",
    "**Induction**: that general rules are acquired by exposure to repeated associations between their elements.\n",
    "\n",
    "**Logical positivism** doctrine holds that all knowledge can be characterized by logical theories connected, ultimately, to **observation sentences** that correspond to sensory inputs; thus logical positivism combines **rationalism and empiricism**.\n",
    "\n",
    "The **confirmation theory** attempted to analyze the acquisition of knowledge from experience by quantifying the degree of belief that should be assigned to logical sentences based on their connection to observations that confirm or disconfirm them.\n",
    "\n",
    "**Utility** to capture the internal, subjective value of an outcome. The modern notion of rational decision making under uncertainty involves maximizing expected utility.\n",
    "\n",
    "**Utilitarianism**: that rational decision making based on maximizing utility should apply to all spheres of human activity, including public policy decisions made on behalf of many individuals. Utilitarianism is a specific kind of **consequentialism**: the idea that what is right and wrong is determined by the expected outcomes of an action.\n",
    "\n",
    "**Rule-based or deontological ethics**, in which “doing the right thing” is determined not by outcomes but by universal social laws that govern allowable actions, such as “don’t lie” or “don’t kill.”\n",
    "\n",
    "The idea of **formal logic**, propositional, or Boolean, logic. The theory of **probability** can be seen as generalizing logic to situations with uncertain information - a consideration of great importance for AI. Bayes’ rule is a crucial tool for AI systems.\n",
    "\n",
    "The formalization of **probability**, combined with the availability of data, led to the emergence of **statistics** as a field.\n",
    "\n",
    "The history of computation is as old as the history of numbers, but the first nontrivial **algorithm** is thought to be Euclid’s algorithm for computing greatest common divisors.\n",
    "\n",
    "**Incompleteness theorem** showed that in any formal theory as strong as Peano arithmetic, there are necessarily true statements that have no proof within the theory.\n",
    "\n",
    "This fundamental result can also be interpreted as showing that some functions on the\n",
    "integers cannot be represented by an algorithm—that is, they cannot be computed. This\n",
    "motivated Alan Turing to try to characterize exactly which functions are **computable**—capable of being computed by an effective procedure. Turing showed that there were some functions that no Turing machine can compute.\n",
    "\n",
    "Although computability is important to an understanding of computation, the notion of\n",
    "**tractability** has had an even greater impact on AI. Roughly speaking, a problem is called intractable if the time required to solve instances of the problem grows exponentially with the size of the instances. \n",
    "\n",
    "The theory of **NP-completeness**, provides a basis for analyzing the tractability of problems: any problem class to which the class of NP-complete problems can be reduced is likely to be intractable. \n",
    "\n",
    "**Decision theory**, which combines probability theory with utility theory, provides a formal and complete framework for individual decisions made under uncertainty. **Game theory**, for some games, a rational agent should adopt policies that are randomized. Unlike decision theory, game theory does not offer an unambiguous prescription for selecting actions. In AI, decisions involving multiple agents are studied under the heading of **multiagent systems**.\n",
    "\n",
    "**human–computer interaction (HCI)** under psychology. **intelligence augmentation**—IA rather than AI. He believed that computers should augment human abilities rather than automate away human tasks. Today we are more likely to see IA and AI as two sides of the same coin, with the former emphasizing human control and the latter emphasizing intelligent behavior on the part of the machine.\n",
    "\n",
    "Each generation of computer hardware has brought an increase in speed and capacity and a decrease in price—a trend captured in **Moore’s law**.\n",
    "\n",
    "Why are **AI and control theory** two different fields, despite the close connections among their founders? The answer lies in the close coupling between the mathematical techniques. Calculus and matrix algebra, the tools of control theory, lend themselves to systems that are describable by fixed sets of continuous variables, whereas AI was founded in part as a way to escape from these perceived limitations. The tools of logical inference and computation allowed AI researchers to consider problems that fell completely outside the control theorist’s purview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148e85d-7141-406e-824e-2b09028ef584",
   "metadata": {},
   "source": [
    "## 1.3 The History of Artificial Intelligence\n",
    "\n",
    "Any computable function could be computed by some network of connected neurons, and that all the logical connectives (AND, OR, NOT, etc.) could be implemented by simple network structures. Donald Hebb (1949) demonstrated a simple updating rule for modifying the connection strengths between neurons. **Hebbian learning**.\n",
    "\n",
    "Two undergraduate students at Harvard built the first neural network computer in 1950. The SNARC.\n",
    "\n",
    "Dartmouth workshop, Newell and Simon, a mathematical theorem-proving\n",
    "system called the Logic Theorist (LT). General Problem Solver,\n",
    "or GPS. Unlike LT, start to imitate human problem-solving protocols. “thinking humanly”. **Physical symbol system** hypothesis, which states that “a physical symbol system has the necessary and sufficient means for general intelligent action.” Any system (human or machine) exhibiting intelligence must operate by manipulating data structures composed of symbols.\n",
    "\n",
    "In 1958, John McCarthy in MIT defined the high-level language **Lisp**, which was to become the dominant AI pro- gramming language for the next 30 years.\n",
    "\n",
    "The first failure reason was that many early AI systems were based primarily on “informed introspection” as to how humans perform a task, rather than on a careful analysis of the task. The second reason for failure was a lack of appreciation of the intractability of many of the problems that AI was attempting to solve. Most of the early problem-solving systems worked by trying out different combinations of steps until the solution was found. This strategy worked initially because microworlds contained very few objects and hence very few possible actions and very short solution sequences.\n",
    "\n",
    "Early experiments in **machine evolution** (**genetic programming**) were based on the undoubtedly correct belief that by making an appropriate series of small mutations to a machine-code program, one can generate a program with good performance for any particular task. Despite thousands of hours of CPU time, almost no progress was demonstrated.\n",
    "\n",
    "The significance of **DENDRAL** was that it was the **first successful knowledge-intensive** system.\n",
    "\n",
    "In the mid-1980s at least four different groups reinvented the **back-propagation** learning algorithm first developed in the early 1960s. \n",
    "\n",
    "In the 1980s, approaches using **hidden Markov models (HMMs)** came to dominate the area. First, they are based on a rigorous mathematical theory. Second, they are\n",
    "generated by a process of training on a large corpus of real speech data.\n",
    "\n",
    "Pearl’s development of Bayesian networks yielded a rigorous and efficient Bayesian network formalism for representing uncertain knowledge as well as practical algorithms for probabilistic reasoning.\n",
    "\n",
    "The term **deep learning** refers to machine learning using multiple layers of simple, adjustable computing elements. Experiments were carried out with such networks as far back as the 1970s, and in the form of **convolutional neural networks** they found some success in hand-written digit recognition in the 1990s. Deep learning relies heavily on powerful hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7553b4-9dc8-4c5e-a5f7-aaf2c5b132c9",
   "metadata": {},
   "source": [
    "## 1.5 Risks and Benefits of AI\n",
    "\n",
    "Instead of focusing on measurable performance in specific applications, AI should return to its roots of striving for, “machines that think, that learn and that create.” They called the effort **human-level AI** or HLAI—a machine should be able to learn to do anything a human can do. The **artificial general intelligence** (AGI). **artificial superintelligence** or ASI—intelligence that far surpasses human ability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
