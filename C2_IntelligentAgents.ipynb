{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff5fbaf3-dbea-4bbd-af40-2b6800cdb658",
   "metadata": {},
   "source": [
    "# Chapter 2 - Intelligent Agents\n",
    "\n",
    "## 2.1 Agents and Environments\n",
    "\n",
    "An **agent** is anything that can be viewed as perceiving its **environment** through **sensors** and acting upon that environment through **actuators**. An agent’s behavior is described by the **agent function** that maps any given **percept sequence** to an action. The agent function is an abstract mathematical description; the agent program is a concrete implementation, running within some physical system.\n",
    "\n",
    "<img src=\"https://i.ibb.co/Y7pHmWbm/image.png\" width=\"250px\">\n",
    "\n",
    "The vacuum-cleaner world, which consists of a robotic vacuum-cleaning agent in a world consisting of squares that can be either dirty or clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab72c3-2cf6-4e4b-b98d-6b41d02d38f8",
   "metadata": {},
   "source": [
    "## 2.2 Good Behavior: The Concept of Rationality\n",
    "\n",
    "**consequentialism**: we evaluate an agent’s behavior by its consequences. This notion of desirability is captured by a **performance measure** that evaluates any given sequence of environment states.\n",
    "\n",
    "We might propose to measure performance by the amount of dirt cleaned up. A rational agent can maximize this performance measure by cleaning up the dirt, then dumping it all on the floor, then cleaning it up again, and so on.\n",
    "\n",
    "### 2.2.2 Rationality\n",
    "\n",
    "- The performance measure that defines the criterion of success.\n",
    "- The agent’s prior knowledge of the environment.\n",
    "- The actions that the agent can perform.\n",
    "- The agent’s percept sequence to date.\n",
    "\n",
    "For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.\n",
    "\n",
    "We need to be careful to distinguish between rationality and **omniscience**. An omniscient agent knows the actual outcome of its actions and can act accordingly; but omniscience is impossible in reality.\n",
    "\n",
    "To the extent that an agent relies on the prior knowledge of its designer rather than on its own percepts and learning processes, we say that the agent lacks **autonomy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c131a8f-aef0-477b-a244-be2e5c595822",
   "metadata": {},
   "source": [
    "## 2.3 The Nature of Environments\n",
    "\n",
    "PEAS (Performance, Environment, Actuators, Sensors) \n",
    "\n",
    "**Fully observable vs. partially observable**: An environment might be partially\n",
    "observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data— an automated taxi cannot see what other drivers are thinking. If the agent has no sensors at all then the environment is **unobservable**.\n",
    "\n",
    "**Single-agent vs. multiagent**\n",
    "\n",
    "In chess, the opponent entity B is trying to maximize its performance measure, which, by the rules of chess, minimizes agent A’s performance measure. Thus, chess is\n",
    "a **competitive** multiagent environment. In the taxi-driving environment, avoiding collisions maximizes the performance measure of all agents, so it is a **partially co-\n",
    "operative** multiagent environment. It is also **partially competitive** because, for example, only one car can occupy a parking space.\n",
    "\n",
    "**Deterministic vs. nondeterministic**. If the next state of the environment is completely determined by the current state and the action executed by the agent(s), then we say the environment is deterministic. In principle, an agent need not worry about uncertainty in a fully observable, deterministic environment.\n",
    "\n",
    "We say that a model of the environment is **stochastic** if it explicitly deals with probabilities and “nondeterministic” if the possibilities are listed without being quantified.\n",
    "\n",
    "**Episodic vs. sequential**: In an episodic task environment, the next episode does not depend on the actions taken in previous episodes. In sequential environments, on the other hand, the current decision could affect all future decisions.\n",
    "\n",
    "**Static vs. dynamic**, If the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is **semidynamic**. Chess, when played with a clock, is semidynamic. Crossword puzzles are static.\n",
    "\n",
    "**Discrete vs. continuous**: The discrete/continuous distinction applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agent.\n",
    "\n",
    "**Known vs. unknown**: The agent’s (or designer’s) state of knowledge about the “laws of physics” of the environment. In a known environment, the outcomes (or outcome probabilities if the environment is nondeterministic) for all actions are given.\n",
    "\n",
    "It is quite possible for a known environment to be partially observable— in solitaire card games, I know the rules but am still unable to see the cards that have not yet been turned over. Conversely, an unknown environment can be fully observable—in a new video game, the screen may show the entire game state but I still don’t know what the buttons do until I try them.\n",
    "\n",
    "**The hardest case is partially observable, multiagent, nondeterministic, sequential, dynamic, continuous, and unknown.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608bab30-c839-4223-956a-b499d52ae7c6",
   "metadata": {},
   "source": [
    "## 2.4 The Structure of Agents\n",
    "\n",
    "agent= architecture + program\n",
    "\n",
    "### 2.4.1 Agent programs\n",
    "\n",
    "The agent programs that we design in this book all have the same skeleton: they take the current percept as input from the sensors and return an action to the actuators.\n",
    "\n",
    "Notice the difference between the agent program, which takes the current percept as input, and the agent function, which may depend on the entire percept history.\n",
    "\n",
    "- Simple reflex agents\n",
    "- Model-based reflex agents\n",
    "- Goal-based agents\n",
    "- Utility-based agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a734fe-f37c-4abd-ae26-60d6cf88312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is instructive to consider why the table-driven approach to agent construction\n",
    "# is doomed to failure. For the taxi, a lookup table with over 10^600,000,000,000\n",
    "# entries for an hour’s driving. Despite all this, TABLE-DRIVEN-AGENT does do\n",
    "# what we want: it implements the desired agent function.\n",
    "\n",
    "def table_driven_agent(percept, percepts=[], table={}):\n",
    "    \"\"\"\n",
    "    Simulates a table-driven agent.\n",
    "    \n",
    "    Arguments:\n",
    "    - percept: the current percept\n",
    "    - percepts: a persistent list of past percepts (maintained across calls)\n",
    "    - table: a dictionary that maps percept sequences to actions\n",
    "    \n",
    "    Returns:\n",
    "    - action: the action found in the table corresponding to the percept sequence\n",
    "    \"\"\"\n",
    "    # Append the new percept to the sequence\n",
    "    percepts.append(percept)\n",
    "\n",
    "    # Look up the action based on the current sequence of percepts\n",
    "    key = tuple(percepts)  # Using tuple to make it hashable for dictionary lookup\n",
    "    action = table.get(key, None)  # Returns None if not found\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20fcfa-d040-4348-ad9b-eeabf18e86a5",
   "metadata": {},
   "source": [
    "### 2.4.2 Simple reflex agents\n",
    "\n",
    "These agents select actions on the basis of the current percept, ignoring the rest of the percept history. Vacuum agent is a simple reflex agent, because its decision is based only on the current location and on whether that location contains dirt.\n",
    "\n",
    "Infinite loops are often unavoidable for simple reflex agents operating in partially observable environments. Escape from infinite loops is possible if the agent can **randomize** its actions. A randomized simple reflex agent might outperform a deterministic simple reflex agent.\n",
    "\n",
    "<img src=\"https://i.ibb.co/fzK64g8n/image.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4074ca6-8ff2-4478-9ac0-cb2fba9dadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflex_vacuum_agent(percept):\n",
    "    location, status = percept\n",
    "    if status == 'Dirty':\n",
    "        return 'Suck'\n",
    "    elif location == 'A':\n",
    "        return 'Right'\n",
    "    elif location == 'B':\n",
    "        return 'Left'\n",
    "\n",
    "# only if the correct decision can be made on the basis of just the current\n",
    "# percept—that is, only if the environment is fully observable.\n",
    "def simple_reflex_agent(percept, rules):\n",
    "    state = interpret_input(percept)\n",
    "    rule = rule_match(state, rules)\n",
    "    action = rule['action']\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572cf87c-7caa-4148-b2c2-2d92219303b3",
   "metadata": {},
   "source": [
    "### 2.4.3 Model-based reflex agents\n",
    "\n",
    "The most effective way to handle partial observability is for the agent to keep track of the part of the world it can’t see now. That is, the agent should maintain some sort of **internal state** that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state.\n",
    "\n",
    "Updating this internal state information as time goes by requires two kinds of knowledge. First, we need some information about how the world changes over time. This knowledge about “how the world works” is called a **transition model** of the world. Second, we need some information about how the state of the world is reflected in the agent’s percepts. This kind of knowledge is called a **sensor model**.\n",
    "\n",
    "<img src=\"https://i.ibb.co/v47H1TNr/image.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e88884-baa5-4d0b-a53f-e889a0d5edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_based_reflex_agent(percept, state, transition_model, sensor_model, rules, action=None):\n",
    "    state = update_state(state, action, percept, transition_model, sensor_model)\n",
    "    rule = rule_match(state, rules)\n",
    "    action = rule['action']\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b335c-ca9f-4e2a-98da-6c407b7c569b",
   "metadata": {},
   "source": [
    "### 2.4.4 Goal-based agents\n",
    "\n",
    "**Search and planning are the** subfields of AI devoted to finding **action sequences** that achieve the agent’s goals.\n",
    "\n",
    "Goal-based agent appears less efficient, it is more flexible because the knowledge that supports its decisions is represented explicitly and can be modified.\n",
    "\n",
    "<img src=\"https://i.ibb.co/MD9YVjv5/image.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc29cfb-35cb-41b1-a890-546673f71317",
   "metadata": {},
   "source": [
    "### 2.4.5 Utility-based agents\n",
    "\n",
    "An agent’s **utility function** is essentially an internalization of the **performance measure**. Provided that the internal utility function and the external performance measure are in agreement, an agent that chooses actions to maximize its **utility** will be rational according to the external performance measure.\n",
    "\n",
    "Perfect rationality is usually unachievable in practice because of computational complexity. Not all utility-based agents are model-based, **a model-free agent** can learn what action is best in a particular situation without ever learning exactly how that action changes the environment.\n",
    "\n",
    "<img src=\"https://i.ibb.co/vvmBdcnQ/image.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2642220e-1bfe-43ed-8e26-66156ce388c1",
   "metadata": {},
   "source": [
    "### 2.4.6 Learning agents\n",
    "\n",
    "The most important distinction is between the **learning element**, which is responsible for making improvements, and the **performance element**, which is responsible for selecting external actions. The performance element is what we have previously considered to be the entire agent.\n",
    "\n",
    "Learning in intelligent agents can be summarized as a process of modification of each component of the agent to bring the components into closer agreement with the available feedback information, thereby improving the overall performance of the agent.\n",
    "\n",
    "<img src=\"https://i.ibb.co/9HTNWY08/image.png\" width=\"250px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77600d-bcc0-45c9-9e2d-6aced6dd7edc",
   "metadata": {},
   "source": [
    "### 2.4.7 How the components of agent programs work\n",
    "\n",
    "We can place the representations along an axis of increasing complexity and expressive power—atomic, factored, and structured.\n",
    "\n",
    "In an **atomic representation** each state of the world is indivisible—it has no internal structure —a single atom of knowledge, a **“black box”** whose only discernible property is that of being identical to or different from another black box. The standard algorithms underlying search and game-playing, hidden Markov models, and Markov decision processes.\n",
    "\n",
    "A **factored representation** splits up each state into a fixed set of **variables or attributes**, each of which can have a **value**. Two different factored states can share some attributes. This makes it much easier to work out how to turn one state into another. Many important areas of AI are based on factored representations, including constraint satisfaction algorithms, propositional logic, planning, Bayesian networks.\n",
    "\n",
    "**Structured representation**, in which objects and their various and varying relationships can be described explicitly. Structured representations underlie relational databases and first-order logic, first-order probability models, and much of natural language understanding.\n",
    "\n",
    "<img src=\"https://i.ibb.co/13G9WJy/image.png\" width=\"500\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
